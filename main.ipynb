{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare\n",
    "\n",
    "先讓結巴分詞套用繁體字典<br>\n",
    "讓之後斷詞表現的好一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/tonylee/2018DSAI/dsai2018_hw2/dict.txt.big.txt ...\n",
      "Dumping model to file cache /tmp/jieba.u78196fc27356e93ab4692506d3f4c643.cache\n",
      "Loading model cost 2.053 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "jieba.set_dictionary('./dict.txt.big.txt')\n",
    "jieba.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Traversal\n",
    "\n",
    "目前 source 擁有 10000 筆資料<br>\n",
    "因此拜訪的方法就很重要\n",
    "\n",
    "在這裡發現 pandas 的 `iterrows()` 的表現非常不好<br>\n",
    "反而使用 list 搭配 `enumerate()` 能有非常好的表現\n",
    "\n",
    "時間上相差幾乎百倍以上\n",
    "\n",
    "我剛開始也是使用 `iterrows()` , 時間上真的慢到懷疑人生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>嘎嫂新髮型宛如高中生 蔡阿嘎合照根本「誘拐未成年少女」</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>美國報告未提證據 俄：操縱大選是無中生有</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>新年搶優惠　通路Apple商品加碼送</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>台灣癌症時鐘加快 每5分26秒就1人罹癌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>藍營胡批兩岸急凍壓垮興航 綠委回嗆「不用腦子的政黨」</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                            1\n",
       "0  1  嘎嫂新髮型宛如高中生 蔡阿嘎合照根本「誘拐未成年少女」\n",
       "1  2         美國報告未提證據 俄：操縱大選是無中生有\n",
       "2  3           新年搶優惠　通路Apple商品加碼送\n",
       "3  4         台灣癌症時鐘加快 每5分26秒就1人罹癌\n",
       "4  5   藍營胡批兩岸急凍壓垮興航 綠委回嗆「不用腦子的政黨」"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = pd.read_csv('./source.csv', header=None)\n",
    "source.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 s, sys: 6.42 ms, total: 7.01 s\n",
      "Wall time: 7.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for index,row in source.iterrows():\n",
    "    title = row[1]\n",
    "    #terms = splitKeyWord(title)\n",
    "    #terms = jieba.cut_for_search(title)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 ms, sys: 0 ns, total: 21.6 ms\n",
      "Wall time: 24.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sourceData = source.iloc[:,1].values\n",
    "\n",
    "for index, title in enumerate(sourceData):\n",
    "    #terms = splitKeyWord(title)\n",
    "    #terms = jieba.cut_for_search(title)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split title\n",
    "\n",
    "這裡除了結巴分詞<br>\n",
    "在同學的建議下, 另外用正規表示式 `RE` 撰寫了兩種分出詞的方法\n",
    "\n",
    "- `splitKeyWord`\n",
    "\n",
    "先將單個字一個個抓出來, 然後從 1 個字到 n 個字分別組合成新的字串\n",
    "成為 query 用的 keyword\n",
    "\n",
    "算是暴力找出長度 n 個字以下的所有組合\n",
    "\n",
    "- `splitKeyWordByRE`\n",
    "\n",
    "與前個方法類似, 但使用 `RE` 本身去取代 for 的遞迴合成字串\n",
    "以加速找出 keyword 的速度\n",
    "\n",
    "---\n",
    "\n",
    "最後是 `splitKeyWordByRE` 快於 `splitKeyWord` 與 `jieba`(結巴)\n",
    "\n",
    "`jieba` 的好處在於只有找到有意義的 keyword, 可是為了找到有意義的單詞需要去比對字典<br>\n",
    "因此需要花費較多時間, 犧牲掉了效能\n",
    "\n",
    "加上該作業需要 100% 正確, 為了保險起見, 選擇 `splitKeyWordByRE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitKeyWord(str, n):\n",
    "    regex = r\"[\\u4e00-\\ufaff]|[0-9]+|[a-zA-Z]+\\'*[a-z]*|[^0-9]\"\n",
    "    matches = re.findall(regex, str, re.UNICODE)\n",
    "    words = list()\n",
    "    for j in range(n):\n",
    "        words.append([])\n",
    "        for i in range(len(matches) - j):\n",
    "            words[j].append(''.join(matches[i:i+j+1]))\n",
    "    return words\n",
    "\n",
    "def splitKeyWordByRE(str,n):\n",
    "    words = [re.findall(r\"[\\u4e00-\\ufaff]{1}|[0-9]+|[a-zA-Z]+\\'*[a-zA-Z]*\", str, re.UNICODE)]        \n",
    "    for i in range(2,n+1):\n",
    "        words.append(re.findall(r\"(?=([\\u4e00-\\ufaff]{%d}))\" % (i), str, re.UNICODE))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "台中、高雄下午反空污大遊行　訴求是這些 \n",
      "\n",
      "using split n word by RE\n",
      "\n",
      " [['台', '中', '高', '雄', '下', '午', '反', '空', '污', '大', '遊', '行', '訴', '求', '是', '這', '些'], ['台中', '高雄', '雄下', '下午', '午反', '反空', '空污', '污大', '大遊', '遊行', '訴求', '求是', '是這', '這些'], ['高雄下', '雄下午', '下午反', '午反空', '反空污', '空污大', '污大遊', '大遊行', '訴求是', '求是這', '是這些']] \n",
      "\n",
      "using split n word\n",
      "\n",
      " [['台', '中', '、', '高', '雄', '下', '午', '反', '空', '污', '大', '遊', '行', '\\u3000', '訴', '求', '是', '這', '些'], ['台中', '中、', '、高', '高雄', '雄下', '下午', '午反', '反空', '空污', '污大', '大遊', '遊行', '行\\u3000', '\\u3000訴', '訴求', '求是', '是這', '這些'], ['台中、', '中、高', '、高雄', '高雄下', '雄下午', '下午反', '午反空', '反空污', '空污大', '污大遊', '大遊行', '遊行\\u3000', '行\\u3000訴', '\\u3000訴求', '訴求是', '求是這', '是這些']] \n",
      "\n",
      "using jieba\n",
      "\n",
      " ['台', '中', '、', '高雄', '下午', '反空', '污大', '遊行', '\\u3000', '訴求', '是', '這些'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "test_sentence = source.iloc[random.randint(0,len(source)-1),1]\n",
    "print(test_sentence,'\\n')\n",
    "print(\"using split n word by RE\\n\\n\",splitKeyWordByRE(test_sentence,3), \"\\n\")\n",
    "print(\"using split n word\\n\\n\",splitKeyWord(test_sentence,3), \"\\n\")\n",
    "print(\"using jieba\\n\\n\",jieba.lcut_for_search(test_sentence), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.62 s, sys: 4.35 ms, total: 1.63 s\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sourceData = source.iloc[:,1].values\n",
    "\n",
    "for index, title in enumerate(sourceData):\n",
    "    terms = splitKeyWordByRE(title,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.04 s, sys: 12.3 ms, total: 3.05 s\n",
      "Wall time: 3.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sourceData = source.iloc[:,1].values\n",
    "\n",
    "for index, title in enumerate(sourceData):\n",
    "    terms = splitKeyWord(title,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 s, sys: 550 µs, total: 16.8 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sourceData = source.iloc[:,1].values\n",
    "\n",
    "for index, title in enumerate(sourceData):\n",
    "    terms = jieba.lcut_for_search(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search engine\n",
    "\n",
    "這邊使用 python 本身具有的 `dictionary` 資料結構<br>\n",
    "是使用 key 經過 hash 得到 index, 藉此加入查找 value 的速度<br>\n",
    "也就是常見的 hash table\n",
    "\n",
    "但這樣的 hash table 會有碰撞的 issue<br>\n",
    "本來想說不同長度的詞分開存進不同的 `dictionary`<br>\n",
    "希望減少碰撞進而加速 insert 的動作\n",
    "\n",
    "但發現全部放進同一個 `dictionary` 也不會造成效能太大的損耗<br>\n",
    "而且有利於後面要查詢的時候不用去判斷要使用哪一個 `dictionary`\n",
    "\n",
    "因此實作選擇使用 `putAll()` and `queryAll()` 這兩個\n",
    "\n",
    "而在這個引擎中使用 keyword 當作 key, 文章標題的索引集合當作 value<br>\n",
    "這樣可以隨意丟入 keyword 查詢到有哪些文章標題含有這個 keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def English(str):\n",
    "    regex = r\"[a-zA-Z]+\\'*[a-z]*\"\n",
    "    matches = re.findall(regex, str, re.UNICODE)\n",
    "    if matches:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "class searchEngine:\n",
    "    def __init__(self, n):\n",
    "        self.dicts = []\n",
    "        self.dictAll = dict()\n",
    "        for i in range(n):\n",
    "            self.dicts.append(dict())\n",
    "    \n",
    "    def put(self, n, word, index):\n",
    "        if n >= len(self.dicts):\n",
    "            return\n",
    "        if word not in self.dicts[n]:\n",
    "            self.dicts[n][word] = set([index])\n",
    "        else:\n",
    "            self.dicts[n][word].add(index)\n",
    "    \n",
    "    def putAll(self, word, index):\n",
    "        if word not in self.dictAll:\n",
    "            self.dictAll[word] = set([index])\n",
    "        else:\n",
    "            self.dictAll[word].add(index)\n",
    "    \n",
    "    def query(self, word):\n",
    "        if English(word):\n",
    "            return self.dicts[0].get(word, set())\n",
    "        else:\n",
    "            return self.dicts[len(word) - 1].get(word, set())\n",
    "    \n",
    "    def queryAll(self, word):\n",
    "        return self.dictAll.get(word, set())\n",
    "\n",
    "n_words = 3\n",
    "engine = searchEngine(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.79 s, sys: 220 ms, total: 8.01 s\n",
      "Wall time: 8.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sourceData = source.iloc[:,1].values\n",
    "\n",
    "for index, title in enumerate(sourceData):\n",
    "    terms = splitKeyWordByRE(title,n_words)\n",
    "    for i in range(n_words):\n",
    "        for term in terms[i]:\n",
    "            engine.put(i, term, index+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.52 s, sys: 168 ms, total: 7.69 s\n",
      "Wall time: 7.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sourceData = source.iloc[:,1].values\n",
    "\n",
    "for index, title in enumerate(sourceData):\n",
    "    terms = splitKeyWordByRE(title,n_words)\n",
    "    for i in range(n_words):\n",
    "        for term in terms[i]:\n",
    "            engine.putAll(term, index+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "query = \"MLB\"\n",
    "\n",
    "a = engine.query(query)\n",
    "\n",
    "b = engine.queryAll(query)\n",
    "\n",
    "print(a == b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query\n",
    "\n",
    "來到關鍵的地方了, 前面利用 `dictionary` 建立好搜尋用的引擎\n",
    "\n",
    "這邊就是要來 query 這個引擎了\n",
    "\n",
    "首先使用 `parseQuery` 去分析哪些是要 query 的 keyword , 哪些是要執行的集合操作\n",
    "\n",
    "那因為作業要求有提到, 一條 query 只會有同一種操作\n",
    "\n",
    "因此把第一個 keyword 的集合找出來, 後面遇到的集合都與第一個集合使用該集合操作\n",
    "\n",
    "最後剩下就會是該 query 的答案\n",
    "\n",
    "轉成 `list` 進行 `sort()`, 並使用 `join()` 變成字串就完成作業要求的答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseQuery(query):\n",
    "    if query[-1] == '\\n':\n",
    "        query = query[:-1]\n",
    "    words = query.split(\" \")\n",
    "    op = None\n",
    "    queryWord = []\n",
    "    for word in words:\n",
    "        if (word == \"and\") or (word == \"or\") or (word == \"not\"):\n",
    "            op = word\n",
    "            continue\n",
    "        queryWord.append(word)\n",
    "    return [queryWord, op]\n",
    "\n",
    "with open('./query.txt', 'r') as queryfile:\n",
    "    querys = queryfile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NBA', '大三元'], 'and']\n"
     ]
    }
   ],
   "source": [
    "print(parseQuery(querys[random.randint(0,len(querys)-1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.43 ms, sys: 31 µs, total: 9.46 ms\n",
      "Wall time: 9.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "answer1 = []\n",
    "\n",
    "for query in querys:\n",
    "    keywords, op = parseQuery(query)\n",
    "    \n",
    "    answer = engine.queryAll(keywords[0])\n",
    "    for keyword in keywords[1:]:\n",
    "        if op == \"and\":\n",
    "            answer = answer & engine.queryAll(keyword)\n",
    "        if op == \"or\":\n",
    "            answer = answer | engine.queryAll(keyword)\n",
    "        if op == \"not\":\n",
    "            answer = answer - engine.queryAll(keyword)\n",
    "    \n",
    "    answer = list(answer)\n",
    "    answer.sort()\n",
    "    if len(answer) == 0:\n",
    "        answer = '0'\n",
    "    else:\n",
    "        answer = ','.join(str(num) for num in answer)\n",
    "    #print(query,\"\\n\",answer,\"\\n\")\n",
    "    answer1.append(answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.56 ms, sys: 0 ns, total: 5.56 ms\n",
      "Wall time: 5.38 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "answer2 = []\n",
    "\n",
    "for query in querys:\n",
    "    keywords, op = parseQuery(query)\n",
    "    \n",
    "    answer = engine.query(keywords[0])\n",
    "    for keyword in keywords[1:]:\n",
    "        if op == \"and\":\n",
    "            answer = answer & engine.query(keyword)\n",
    "        if op == \"or\":\n",
    "            answer = answer | engine.query(keyword)\n",
    "        if op == \"not\":\n",
    "            answer = answer - engine.query(keyword)\n",
    "    \n",
    "    answer = list(answer)\n",
    "    answer.sort()\n",
    "    if len(answer) == 0:\n",
    "        answer = '0'\n",
    "    else:\n",
    "        answer = ','.join(str(num) for num in answer)\n",
    "    #print(query,\"\\n\",answer,\"\\n\")\n",
    "    answer2.append(answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "電玩 and 宅男\n",
      " True\n",
      "美國 and 北韓\n",
      " True\n",
      "美國 or 台灣 or 中國\n",
      " True\n",
      "母親節 and 禮物\n",
      " True\n",
      "職籃 or 職棒\n",
      " True\n",
      "台灣 and GDP\n",
      " True\n",
      "NBA or MLB\n",
      " True\n",
      "NBA and 大三元\n",
      " True\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(querys)):\n",
    "    print(querys[i], answer1[i] == answer2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- 華哥\n",
    "- 翁帥哥\n",
    "- [使用 Python 模块 re 实现解析小工具](https://www.ibm.com/developerworks/cn/opensource/os-cn-pythonre/index.html)\n",
    "- [Python字典对象实现原理](https://foofish.net/python_dict_implements.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
